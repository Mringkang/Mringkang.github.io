---
title: "python爬虫：爬取图片进阶篇"
layout: post
date: 2017-09-26
image: /assets/images/markdown.jpg
headerImage: false
tag:
- iOS
category: blog
---



前两篇文章总感觉自己循环遍历太多，今天看了看Beautiful Soup这个库的文档其实在只有一个节点的情况下没有必要用findAll函数，这就省去了多次遍历，简化流程。

今天目标站[妹子图](http://www.mmjpg.com/)，博眼球真是费了心。

**三步走**

* 获取这个网址的html，解析拿到图集的网址
* 访问图集的地址，找到图片的下载地址
* 下载图片


![](https://ws1.sinaimg.cn/large/9e1008a3ly1fjwzb0180nj20s10kfqi9.jpg)

**获取这个网址的html，解析拿到图集的网址**

一共15套图的地址

	 html = urllib2.urlopen('http://www.mmjpg.com').read()
    # 解析html
    soup = BeautifulSoup(html)
    spanResult = soup.findAll('span',attrs={"class":"title"})
	
    for a in spanResult:
        name = a.find('a').string
        href = a.find('a').get('href')
        


![](https://ws1.sinaimg.cn/large/9e1008a3ly1fjx0zk4s6wj20sc0mnqfy.jpg)

**访问图集的地址，找到图片的下载地址**

每套路40张图片

	 linkreq = urllib2.Request(href)
    linkresponse = urllib2.urlopen(linkreq)
    htmlres = linkresponse.read()
    soups = BeautifulSoup(htmlres)
    imageResult = soups.findAll('div',attrs={"id":"content"})
    # print imageResult
	
    for div in imageResult:
        # 得到所有的img标签
        image = div.find('img')
        # print image
        link = image.get('src')
        # print link
	
	
**下载图片**


	# 拿到链接截取拼接
	for x in range(1,41):
	urlresult = '%s%s%s' % (link[:-5], x, '.jpg')
	filePath = '/Users/kangbing/Desktop/image/%s%s.jpg' % (name,x)
	urllib.urlretrieve(urlresult,filePath)
	print urlresult
	print filePath.encode('utf-8')
	
	

![](https://ws1.sinaimg.cn/large/9e1008a3ly1fjwzbse6nwg20ac0isx6p.gif)




最终代码：

	#!/usr/bin/python
	#-*- coding: utf-8 -*-
	#encoding=utf-8

	import urllib2
	import urllib
	import os
	from BeautifulSoup import BeautifulSoup
	
	def downLoadAllImage():
	    html = urllib2.urlopen('http://www.mmjpg.com').read()
	    # 解析html
	    soup = BeautifulSoup(html)
	    spanResult = soup.findAll('span',attrs={"class":"title"})
	
	    for a in spanResult:
	        name = a.find('a').string
	        href = a.find('a').get('href')
	        linkreq = urllib2.Request(href)
	        linkresponse = urllib2.urlopen(linkreq)
	        htmlres = linkresponse.read()
	        soups = BeautifulSoup(htmlres)
	        imageResult = soups.findAll('div',attrs={"id":"content"})
	        # print imageResult
	
	        for div in imageResult:
	            # 得到所有的img标签
	            image = div.find('img')
	            # print image
	            link = image.get('src')
	            # print link
	            # 拿到链接截取拼接
	            for x in range(1,41):
	                urlresult = '%s%s%s' % (link[:-5], x, '.jpg')
	                filePath = '/Users/kangbing/Desktop/image/%s%s.jpg' % (name,x)
	                urllib.urlretrieve(urlresult,filePath)
	                print urlresult
	                print filePath.encode('utf-8')
	
	
	if __name__ == '__main__':
	    downLoadAllImage()
	    


![](https://ws1.sinaimg.cn/large/9e1008a3ly1fjx3sz7616j20a70evjsm.jpg)

温馨提示:
**身体不好的学习技术就好了，图片还是删了吧。。。**
